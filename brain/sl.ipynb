{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dynet as dy\n",
    "import numpy as np\n",
    "from time import time\n",
    "import random, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward:\n",
    "    def __init__(self, model, num_input, num_hidden, num_out, act):\n",
    "        pc = model.add_subcollection()\n",
    "        self.W1 = model.add_parameters((num_hidden, num_input))\n",
    "        self.W2 = model.add_parameters((num_out, num_hidden))\n",
    "        self.b1 = model.add_parameters((num_hidden))\n",
    "        self.b2 = model.add_parameters((num_out))\n",
    "        self.pc = pc #?\n",
    "        self.act = act\n",
    "        self.spec = (num_input, num_hidden, num_out, act)\n",
    "\n",
    "    def __call__(self, input_exp):\n",
    "        W1 = self.W1.expr()\n",
    "        W2 = self.W2.expr()\n",
    "        b1 = self.b1.expr()\n",
    "        b2 = self.b2.expr()\n",
    "\n",
    "        g = self.act\n",
    "        return W2 * g(W1 * input_exp + b1) + b2\n",
    "\n",
    "    def param_collection(self):\n",
    "        return self.pc #?\n",
    "\n",
    "    @staticmethod\n",
    "    def from_spec(spec, model):\n",
    "        num_input, num_hidden, num_out, act = spec\n",
    "        return FeedForward(model, num_input, num_hidden, num_out, act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Shortlister:\n",
    "    def __init__(self, context, non_personalized=False):\n",
    "\n",
    "        self.model = dy.ParameterCollection()\n",
    "\n",
    "        self.opts = context.opts\n",
    "        self.char_vocab = context.char_vocab\n",
    "        self.word_vocab = context.word_vocab\n",
    "        self.skill_vocab = context.skill_vocab\n",
    "\n",
    "        self.char_count = context.char_count\n",
    "        self.word_count = context.word_count\n",
    "        self.skill_count = context.skill_count\n",
    "\n",
    "        cdim = self.opts['dim_char_embeddings']\n",
    "        wdim = self.opts['dim_word_embeddings']\n",
    "        ldim = self.opts['dim_lstm_outputs']\n",
    "\n",
    "        self.dim_lstm_outputs = self.opts['dim_lstm_outputs']\n",
    "        self.char_lookup = self.model.add_lookup_parameters((self.char_vocab.size()+1, cdim))\n",
    "        self.word_lookup = self.model.add_lookup_parameters((self.word_vocab.size()+1, wdim))\n",
    "        self.skill_lookup = self.model.add_lookup_parameters((self.skill_vocab.size()+1, ldim * 2))# why +1 , why * 2\n",
    "\n",
    "        # # Hidden layer dimension\n",
    "        dim_wlstm_input = 2 * cdim + wdim\n",
    "        dim_wlstm_output = ldim\n",
    "\n",
    "        # LSTM parameters\n",
    "        self.char_fwd_lstm = dy.CoupledLSTMBuilder(1, cdim, cdim, self.model)\n",
    "        self.char_bwd_lstm = dy.CoupledLSTMBuilder(1, cdim, cdim, self.model)\n",
    "        self.word_fwd_lstm = dy.CoupledLSTMBuilder(1, dim_wlstm_input, dim_wlstm_output, self.model)\n",
    "        self.word_bwd_lstm = dy.CoupledLSTMBuilder(1, dim_wlstm_input, dim_wlstm_output, self.model)\n",
    "        #\n",
    "        # FeedForward parameters\n",
    "        dim_ff_input = 2 * dim_wlstm_output\n",
    "        dim_ff_output = self.skill_vocab.size()\n",
    "        if self.opts['act'] == 'relu':\n",
    "            act = dy.rectify\n",
    "        elif self.opts['act'] == 'selu':\n",
    "            act = dy.selu\n",
    "        elif self.opts['act'] == 'tanh':\n",
    "            act = dy.tanh\n",
    "        else:\n",
    "            assert False, \"not supported activation function\"\n",
    "\n",
    "        if non_personalized == True:\n",
    "            self.opts['non_personalized'] = True\n",
    "\n",
    "        ff_size_multiple = 2 if 'non_personalized' not in self.opts else 1 # Non-personalized models do not have this option\n",
    "\n",
    "        self.feed_forward = FeedForward(self.model, ff_size_multiple * dim_ff_input, ff_size_multiple * dim_ff_input, dim_ff_output, act)\n",
    "        #\n",
    "        # Scale for skill enablement\n",
    "        self.scale = self.model.add_parameters((dim_ff_output)) if 'non_personalized' not in self.opts else None\n",
    "        self.load()\n",
    "\n",
    "    # Get embedding from lookup parameter\n",
    "    def get_emb(self, cnts, lookup, idx):\n",
    "        # Unseen type\n",
    "        if idx not in cnts:\n",
    "            return lookup[0]\n",
    "        return lookup[idx+1]\n",
    "\n",
    "    def compute_skill_summary(self, datum, utterance_expr):\n",
    "        dim_skill_summary = 2 * self.dim_lstm_outputs\n",
    "        att_weights, att_skills = [], []\n",
    "\n",
    "        if len(datum.enabled_skills_idx) == 0:\n",
    "            return dy.zeroes((dim_skill_summary, 1))\n",
    "\n",
    "        if len(datum.enabled_skills_idx) == 1:\n",
    "            for s in datum.enabled_skills_idx:\n",
    "                return self.get_emb(self.skill_count, self.skill_lookup, s)\n",
    "\n",
    "        skill_count = self.skill_count\n",
    "        skill_lookup = self.skill_lookup\n",
    "\n",
    "        for s in datum.enabled_skills_idx:\n",
    "            skill_emb = self.get_emb(skill_count, skill_lookup, s)\n",
    "            att_skills.append(skill_emb)\n",
    "            att_weight = dy.dot_product(utterance_expr, skill_emb)\n",
    "            att_weights.append(att_weight)\n",
    "\n",
    "        # Non-empty enabled skills and if len(att_weights) > 0:\n",
    "        normalized_weights = dy.softmax(dy.concatenate(att_weights))\n",
    "        weighted_att_skills = [att_skills[i]*normalized_weights[i] for i in xrange(len(att_weights))]\n",
    "        skill_summary = dy.esum(weighted_att_skills)\n",
    "\n",
    "        return skill_summary\n",
    "\n",
    "    def disable_dropout(self):\n",
    "        self.char_fwd_lstm.disable_dropout()\n",
    "        self.char_bwd_lstm.disable_dropout()\n",
    "        self.word_fwd_lstm.disable_dropout()\n",
    "        self.word_bwd_lstm.disable_dropout()\n",
    "\n",
    "    def build_graph(self, datum):\n",
    "        char_vocab = self.char_vocab\n",
    "        word_vocab = self.word_vocab\n",
    "        skill_vocab = self.skill_vocab\n",
    "\n",
    "        char_count = self.char_count\n",
    "        word_count = self.word_count\n",
    "        skill_count = self.skill_count\n",
    "\n",
    "        char_lookup = self.char_lookup\n",
    "        word_lookup = self.word_lookup\n",
    "        skill_lookup = self.skill_lookup\n",
    "\n",
    "        char_fwd_lstm = self.char_fwd_lstm\n",
    "        char_bwd_lstm = self.char_bwd_lstm\n",
    "\n",
    "        word_fwd_lstm = self.word_fwd_lstm\n",
    "        word_bwd_lstm = self.word_bwd_lstm\n",
    "\n",
    "        scale = dy.parameter(self.scale) if 'non_personalized' not in self.opts else None\n",
    "\n",
    "        wlstm_input_vec = []\n",
    "        for word in datum.words:\n",
    "            c_fwd = char_fwd_lstm.initial_state()\n",
    "            c_bwd = char_bwd_lstm.initial_state()\n",
    "            clstm_input_vec = [self.get_emb(char_count, char_lookup, char_vocab.index_of(c)) for c in word]\n",
    "            c_fwd_outs = c_fwd.transduce(clstm_input_vec)\n",
    "            c_bwd_outs = c_bwd.transduce(reversed(clstm_input_vec))\n",
    "            w_emb = self.get_emb(word_count, word_lookup, word_vocab.index_of(word))\n",
    "            wlstm_input = dy.concatenate([c_fwd_outs[-1], c_bwd_outs[-1], w_emb])\n",
    "            wlstm_input_vec.append(wlstm_input)\n",
    "\n",
    "        w_fwd = word_fwd_lstm.initial_state()\n",
    "        w_bwd = word_bwd_lstm.initial_state()\n",
    "        w_fwd_out = w_fwd.transduce(wlstm_input_vec)\n",
    "        w_bwd_out = w_bwd.transduce(reversed(wlstm_input_vec))\n",
    "\n",
    "        utterance_expr = dy.concatenate([w_fwd_out[-1], w_bwd_out[-1]])\n",
    "\n",
    "        # Skill summary\n",
    "        ff_input = utterance_expr\n",
    "        if 'non_personalized' not in self.opts:\n",
    "            skill_summary_expr = self.compute_skill_summary(datum, utterance_expr)\n",
    "            ff_input = dy.concatenate([utterance_expr, skill_summary_expr])\n",
    "\n",
    "        scores = self.feed_forward(ff_input)\n",
    "\n",
    "        if self.opts.get('use_skill_bias', False) == True:\n",
    "            scores += dy.cmult(scale, dy.inputVector(datum.skill_enablement)) # add skill bias (n-hot vector of the skill enablement)\n",
    "\n",
    "        return scores\n",
    "\n",
    "    @property\n",
    "    def model(self):\n",
    "        return self.model\n",
    "\n",
    "    def load(self):\n",
    "        self.model.populate(self.opts['model_path'] + '/model.bin')\n",
    "\n",
    "    def predict(self, datum):\n",
    "        scores = self.build_graph(datum)\n",
    "        scores = dy.softmax(scores)\n",
    "        probs = scores.npvalue()\n",
    "        pred = np.argmax(probs)\n",
    "        return pred, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
