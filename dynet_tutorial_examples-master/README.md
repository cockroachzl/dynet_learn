# Practical Neural Networks for NLP

A tutorial given by [Chris Dyer](http://www.cs.cmu.edu/~cdyer/), [Yoav Goldberg](https://www.cs.bgu.ac.il/~yoavg/uni/), and [Graham Neubig](http://www.phontron.com/) at EMNLP 2016 in Austin. The tutorial covers the basic of neural networks for NLP, and how to implement a variety of networks simply and efficiently in the [DyNet](https://www.github.com/clab/dynet) toolkit.

* [Slides, part 1: Basics](http://phontron.com/slides/emnlp2016-dynet-tutorial-part1.pdf)
    * Computation graphs and their construction
    * Neural networks in DyNet
    * Recurrent neural networks
    * Minibatching
    * Adding new differentiable functions

* [Slides, part 2: Case studies in NLP](http://phontron.com/slides/emnlp2016-dynet-tutorial-part2.pdf)
    * Tagging with bidirectional RNNs and character-based embeddings
    * Transition-based dependency parsing
    * Structured prediction meets deep learning
