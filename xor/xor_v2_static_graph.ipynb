{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Static network\n",
    "import dynet as dy\n",
    "\n",
    "# multilayer perceptron with a single hidden layer\n",
    "# input 2 nodes\n",
    "# hidden layer: 8 nodes, activation: tanh\n",
    "# output layer: 1 node\n",
    "# Ïƒ(V(tanh(Wx+b)))\n",
    "# x: 2x1\n",
    "# W: 8x2\n",
    "# b: 8 vector\n",
    "# V: 1x8\n",
    "\n",
    "# define the parameters\n",
    "m = dy.ParameterCollection()\n",
    "pW = m.add_parameters((8,2)) # _dynet.Parameters\n",
    "pV = m.add_parameters((1,8))\n",
    "pb = m.add_parameters((8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_dynet.ComputationGraph at 0x10ab6bb40>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dy.renew_cg() # new computation graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the parameters to the graph\n",
    "# create Expression representing the network \n",
    "#(the network will include the Expressions \n",
    "# for the Parameters defined in the parameter collection)\n",
    "W = dy.parameter(pW) #W is of type _dynet.Expression\n",
    "V = dy.parameter(pV)\n",
    "b = dy.parameter(pb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W.dimension: ((8, 2), 1)\n",
      "b.dimension: ((8,), 1)\n",
      "V.dimension: ((1, 8), 1)\n"
     ]
    }
   ],
   "source": [
    "print('W.dimension: {}'.format(W.dim()))\n",
    "print('b.dimension: {}'.format(b.dim()))\n",
    "print('V.dimension: {}'.format(V.dim()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the network\n",
    "x = dy.vecInput(2) # an input vector of size 2. _dynet._vecInputExpression\n",
    "output = dy.logistic(V*(dy.tanh((W*x)+b))) # output is _dynet.Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '_dynet._vecInputExpression'>\n",
      "<class '_dynet.Expression'>\n"
     ]
    }
   ],
   "source": [
    "print(type(x))\n",
    "print(type(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((8,), 1)\n",
      "((8,), 1)\n",
      "((1,), 1)\n",
      "((1,), 1)\n",
      "((1,), 1)\n"
     ]
    }
   ],
   "source": [
    "print((W*x+b).dim())\n",
    "print(dy.tanh((W*x+b)).dim())\n",
    "print((V*dy.tanh((W*x+b))).dim())\n",
    "print(dy.logistic(V*dy.tanh((W*x+b))).dim())\n",
    "print(output.dim())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6098086833953857"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can now query our network\n",
    "x.set([0,0])\n",
    "output.value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to be able to define a loss, so we need an input expression to work against.\n",
    "y = dy.scalarInput(0) # this will hold the correct answer\n",
    "loss = dy.binary_log_loss(output, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '_dynet._inputExpression'>\n",
      "<class '_dynet.Expression'>\n"
     ]
    }
   ],
   "source": [
    "print(type(y))\n",
    "print(type(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = dy.SimpleSGDTrainer(m)\n",
    "# trainer = dy.CyclicalSGDTrainer(m)\n",
    "# trainer = dy.AdamTrainer(m)\n",
    "# trainer = dy.AdagradTrainer(m)\n",
    "trainer = dy.RMSPropTrainer(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_dynet.RMSPropTrainer"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0010000000474974513"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the loss before step is: 0.24690616130828857\n",
      "the loss after step is: 0.24105413258075714\n"
     ]
    }
   ],
   "source": [
    "# single step optimization/training\n",
    "\n",
    "x.set([1,0])\n",
    "y.set(1)\n",
    "loss_value = loss.value() # this performs a forward through the network.\n",
    "print(\"the loss before step is:\",loss_value)\n",
    "\n",
    "# now do an optimization step\n",
    "loss.backward()  # compute the gradients\n",
    "trainer.update()\n",
    "\n",
    "# see how it affected the loss:\n",
    "loss_value = loss.value(recalculate=True) # recalculate=True means \"don't use precomputed value\"\n",
    "print(\"the loss after step is:\",loss_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training data set\n",
    "def create_xor_instances(num_rounds=2000):\n",
    "    questions = []\n",
    "    answers = []\n",
    "    for round in range(num_rounds):\n",
    "        for x1 in 0,1:\n",
    "            for x2 in 0,1:\n",
    "                answer = 0 if x1==x2 else 1\n",
    "                questions.append((x1,x2))\n",
    "                answers.append(answer)\n",
    "    return questions, answers\n",
    "\n",
    "questions, answers = create_xor_instances()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\n",
      "8000\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[0, 1, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "print(len(questions))\n",
    "print(len(answers))\n",
    "print(questions[:4])\n",
    "print(answers[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average loss is: 0.7972901383042336\n",
      "average loss is: 0.7745968557894229\n",
      "average loss is: 0.7570578746000926\n",
      "average loss is: 0.7432997459173203\n",
      "average loss is: 0.7323096290230751\n",
      "average loss is: 0.7233400035897891\n",
      "average loss is: 0.715852968096733\n",
      "average loss is: 0.7094636897742749\n",
      "average loss is: 0.70389404296875\n",
      "average loss is: 0.6989397900104523\n",
      "average loss is: 0.6944485448707234\n",
      "average loss is: 0.6903049652278423\n",
      "average loss is: 0.6864206374608554\n",
      "average loss is: 0.6827270120382309\n",
      "average loss is: 0.6791703767776489\n",
      "average loss is: 0.6757082035019993\n",
      "average loss is: 0.6723064525337781\n",
      "average loss is: 0.668937573366695\n",
      "average loss is: 0.6655790155498605\n",
      "average loss is: 0.6622121162116528\n",
      "average loss is: 0.6588212782854126\n",
      "average loss is: 0.6553933482820338\n",
      "average loss is: 0.6519171639629032\n",
      "average loss is: 0.6483832151691119\n",
      "average loss is: 0.6447833909749985\n",
      "average loss is: 0.6411107977766257\n",
      "average loss is: 0.6373596195269514\n",
      "average loss is: 0.633525016978383\n",
      "average loss is: 0.6296030541124015\n",
      "average loss is: 0.625590640048186\n",
      "average loss is: 0.6214854873861037\n",
      "average loss is: 0.6172860765084625\n",
      "average loss is: 0.6129916276985948\n",
      "average loss is: 0.6086020742619739\n",
      "average loss is: 0.604118039224829\n",
      "average loss is: 0.5995408082670636\n",
      "average loss is: 0.5948723037017358\n",
      "average loss is: 0.5901150578887839\n",
      "average loss is: 0.5852721777634743\n",
      "average loss is: 0.5803473094925284\n",
      "average loss is: 0.5753446018841208\n",
      "average loss is: 0.5702686568988221\n",
      "average loss is: 0.565124488154123\n",
      "average loss is: 0.5599174690856175\n",
      "average loss is: 0.5546532741387685\n",
      "average loss is: 0.5493378267728765\n",
      "average loss is: 0.5439772422738531\n",
      "average loss is: 0.5385777693986893\n",
      "average loss is: 0.5331457359571846\n",
      "average loss is: 0.5276874961704016\n",
      "average loss is: 0.5222093798862952\n",
      "average loss is: 0.5167176458855661\n",
      "average loss is: 0.5112184382384678\n",
      "average loss is: 0.5057177502634349\n",
      "average loss is: 0.500221390852874\n",
      "average loss is: 0.4947349600108074\n",
      "average loss is: 0.489263821607619\n",
      "average loss is: 0.4838130884925867\n",
      "average loss is: 0.47838760532438757\n",
      "average loss is: 0.472991939128687\n",
      "average loss is: 0.46763037317661477\n",
      "average loss is: 0.4623069048720983\n",
      "average loss is: 0.4570252427328674\n",
      "average loss is: 0.4517888100264827\n",
      "average loss is: 0.44660074952130135\n",
      "average loss is: 0.44146393036300485\n",
      "average loss is: 0.4363809531704703\n",
      "average loss is: 0.43135416121769915\n",
      "average loss is: 0.4263856492369719\n",
      "average loss is: 0.4214772742417242\n",
      "average loss is: 0.41663066699786083\n",
      "average loss is: 0.41184724237899195\n",
      "average loss is: 0.40712821133103066\n",
      "average loss is: 0.40247459177053657\n",
      "average loss is: 0.3978872201892237\n",
      "average loss is: 0.39336676215189265\n",
      "average loss is: 0.3889137227310763\n",
      "average loss is: 0.38452845685721304\n",
      "average loss is: 0.3802111792582075\n",
      "average loss is: 0.3759619737669127\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "total_loss = 0\n",
    "seen_instances = 0\n",
    "for question, answer in zip(questions, answers):\n",
    "    x.set(question)\n",
    "    y.set(answer)\n",
    "    seen_instances += 1\n",
    "    total_loss += loss.value() # forward\n",
    "    loss.backward()\n",
    "    trainer.update()\n",
    "    if (seen_instances > 1 and seen_instances % 100 == 0):\n",
    "        print(\"average loss is:\",total_loss / seen_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1 0.9514448046684265\n",
      "1,0 0.9690647125244141\n",
      "0,0 0.012001067399978638\n",
      "1,1 0.05876074358820915\n"
     ]
    }
   ],
   "source": [
    "x.set([0,1])\n",
    "print(\"0,1\",output.value())\n",
    "\n",
    "x.set([1,0])\n",
    "print(\"1,0\",output.value())\n",
    "\n",
    "x.set([0,0])\n",
    "print(\"0,0\",output.value())\n",
    "\n",
    "x.set([1,1])\n",
    "print(\"1,1\",output.value())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.07663822, -2.77607584],\n",
       "       [ 2.17588735, -3.42900205],\n",
       "       [ 3.63992882,  3.72946382],\n",
       "       [ 0.06008036, -1.52189255],\n",
       "       [-3.54680562,  2.17219925],\n",
       "       [ 1.74713027, -2.989784  ],\n",
       "       [-0.82024181, -0.7913931 ],\n",
       "       [ 4.60517788,  4.43824816]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.26245904,  0.91464269,  1.09329867,  0.70665795,  1.29633462,\n",
       "         1.35833979,  0.8188495 ,  1.72384453]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V.value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6605279445648193,\n",
       " -0.45908284187316895,\n",
       " -0.1739916205406189,\n",
       " 0.2111283540725708,\n",
       " -0.35437655448913574,\n",
       " -0.33467215299606323,\n",
       " 0.3537399470806122,\n",
       " -0.7486727237701416]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.save('models/xor_static_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
